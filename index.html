<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>My Resume</title>
</head>

<body>
    <header>
        <h1>Curriculum Vitae - Nicholas Hoffman</h1>
    </header>
    <a href="./public/about.html">
        <img src="./assets/images/bw_prof_pic.png" class="center" alt="Me" width="206">
      </a>
    <hr/>
    <main>
        <section>
            <h2>Summary</h2>
            <p>
                Analytics professional with 7+ years in high-performance data systems, specializing in data modeling, ETL pipeline 
                development, and analytics-driven decision-making. Proven ability to optimize cost structures, automate workflows, and
                enhance data pipelines for large-scale transactions, sucessfully scaling data initiatives at high-growth fintech platforms 
                like Stripe and Marqeta. Deep expertise in SQL and Python, and the modern data stack
                (Redshift, Snowflake, Airflow, Looker), with expertise in data warehousing, operational analytics, and backend engineering
                development. Adept at working cross-functionally to drive data-driven insights and build scalable, high-impact solutions.
            </p>
        </section>
        <hr/>
        <section>
            <h2>Education</h2>
            <h3>The College of William & Mary</h3>
            <p><i>Master of Science | Business Analytics </i> - Williamsburg, VA (2017)</p>
            <p>Relevant Coursework: Artificial Intelligence • Machine Learning I & II • Database Management • Big Data • Optimization • Data Visualization</p>
            <p>Capstone Project: Implementing Natural Language Processing (NLP) to automate the association of project descriptions to one or more of 17 project goals </p>
            <p><i>Bachelor of Science | Applied Mathemetics and Economics </i> - Williamsburg, VA (2016)</p>
            <p>Concentration: Probability & Statistics</p>
            <p>Relevant Coursework: Econometrics • Statistical Analysis • Time Series Analysis • Data Structures • Computational Problem Solving</p>
        </section>
        <hr/>
        <section>
            <h2>Skills & Exptertise</h2>
            <p><strong>Programming Languages</strong></p>
            <ul>
                <li>SQL (Expert)</li>
                <li>Python (Advanced)</li> 
                <li>Bash</li>
                <li>Ruby</li>
                <li>HTML</li>
            </ul>
            <p><strong>Data Tools</strong></p>
            <ul>
                <li>Airflow</li>
                <li>Snowflake</li>
                <li>Redshift</li>
                <li>Looker</li>
                <li>QlikSense</li>
                <li>Streamlit</li>
            </ul>
            <strong>Analytics & Reporting</strong> 
            <ul>
                <li>Data Modeling</li>
                <li>ETL Pipeline Development</li>
                <li>Financial Reconciliation</li>
                <li>Operational Analytics & Automation</li>
                <li>Query Optimization</li>
                <li>Quantitative Data Storytelling</li>
                <li>Business Intelligence</li>
                <li>Data Governance</li>
            </ul>
            <strong>Soft Skills</strong> 
            <ul>
                <li>Cross-Functional Collaboration</li>
                <li>Stakeholder Management</li>
                <li>Junior Analyst Mentorship</li>
                <li>Technical Incident Resolution</li>
                <li>Payment Systems Expertise</li>
                <li>Technical Documentation</li>
                <li>Product Development</li>
            </ul>
        </section>
        <hr/>
        <section>
            <h2>Experience</h2>
            <h3>Stripe (2022 - 2025)</h3>
            <p><strong>Cost Platform Technical Operations</strong> - South San Francisco, CA</p>
            <p>Named strategic analytics leader for Cost Platform team, being promoted from L2 to L3 in 2024 with consistently strong performance reviews. 
                Deliver critical insights and develop technical solutions that drive business decisions across payment operations and financial reporting. 
                Operate in dynamic role bridging data analytics and integration reliability engineering, identifying inefficiencies and bugs while building internal tooling and dashboards. 
                Collaborate with diverse stakeholders: Legal Compliance, Core Engineering, Backend Engineering, Data Engineering, and Customer Success.</p>
            <p><i>Financial Data Analyst (Q1 2022 - Q1 2025)</i></p>
            <ul>
                <li>Built automated SQL modeling and reporting for <strong>$60M+/year</strong> of new incentive payments, streamlining cross-functional
                    operations between Finance & Strategy, Accounting, Partnerships and Legal teams.</li>
                <li>Led 12 person engineering and ops Network Cost Updates team handling $60M+ in network cost increases across 350M+ transactions annually.</li>
                <li>Devloped complex transactional SQL model of global customer financial forecast of upcoming Network Cost Updates for 2023 cycle.</li>
                <li>Implemented data quality checks and monitoring tools to ensure data integrity and system performance.</li>
                <li>Mentored junior analysts & contractors on optimizing SQL models, modeling interchange, and designing insightful dashboards.</li>
            </ul>
            <p><i>Integration Reliability Engineer (Q3 2022 - Q1 2025)</i></p>
            <ul>
                <li><strong>Eliminated 98% of processing failures ($5.4M)</strong> by diagnosing and resolving interchange tree bugs across multiple regions while leading cross-team initiative. 
                    Developed new interchange trees and implemented supporting Ruby scripts and functions to establish a more reliable transaction processing framework.
                    </li>
                <li>Established interchange domain expert for monitoring, diagnosing, and correcting issues with processing in Ruby interchange trees.
                    Incident manager for multiple S1 ($1M+) incidents, leading cross-functional teams to resolve issues, quantify impact, and implement long-term solutions.
                </li>
                <li>Developed internal Python tool to service custom searches of network fee model data via Spark SQL, Airflow & Streamlit.
                    Wrote queries joining network scheme fee modeling metadata on accuracy, modeling logic, and changes with aggregated transaction data for customer billing.
                    Significantly improved data accessibility for TechOps runner responding to incidents and customer billing inquiries.
                </li>
            </ul>

            <h3>Marqeta (2018-2022)</h3>
            <p><strong>Financial Data Services</strong> - Oakland, CA</p>
            <p><i>Lead Data Scientist (Q4 2021 - Q1 2022)</i></p>
            <p>Led critical data initiatives focused on financial reconciliation and automated reporting systems for payment processing.</p>
            <ul>
                <li><strong>Recovered $550K of reconciled customer funds</strong> and preserved vital business relationship by engineering financial reconciliation systems for critical UK partnership. 
                    Built Python tool that automated three-way transaction reconciliation among customer accounts, network reports, and bank balances for the first time at MQ. 
                    Executed as high-priority ad hoc project, working through holiday season to ensure timely delivery.</li>
            </ul>
            <p><i>Senior Data Scientist (Q4 2019 - Q4 2021)</i></p>
            <p>Spearheaded analytics engineering initiatives for financial services, focusing on data pipeline development, cost optimization, and reconciliation automation across payment processing systems.</p>
            <ul>
                <li>Designed and implemented ETL pipelines with core processing engineering team using Airflow and Python for network data files, standardizing diverse clearing and settlement files into unified client reports.
                    Served as functional product expert for a major banking client, collaborating with engineering teams on event intake automation and with product managers to streamline ledgering processes.
                </li>
                <li>Optimized financial reporting infrastructure by restructuring SQL business logic for quarterly network reports, collaborating with engineering teams to build parsing logic for new network data sources. 
                    Saved $356K while achieving data parity and regulatory compliance.
                </li>
                <li>Mentored settlement analysts on SQL best practices and how to use Bash, Python and Snowflake to automate daily work.</li>
                <li>Migrated data warehouse and reporting systems from Redshift to Snowflake, standardizing stakeholder reporting logic and restructuring MQ's data warehouse.</li>
            </ul>
            <p><i>Data Analyst (Q3 2018 - Q4 2019)</i></p>
            <p>Improved operational efficiency by transforming manual financial reporting processes into automated data pipelines while facilitating three-way reconciliation between banks, networks, and internal ledger systems. Conducted stakeholder interviews across finance and accounting departments to ensure accurate data mapping and workflow optimization.</p>
            <ul>
                <li>Architected end-to-end automation solution migrating monthly and quarterly report production to Github with Python/Airflow, 
                    reducing accounting close time 83% (from 30 to five days) and establishing reliable data governance protocols. 
                    Implemented Slack alert integration for real-time notification of report completion. 
                    Cut report production time from one week to < 1 day, enabling earlier initiation of accounting close process.
                </li>
                <li><strong>Enhanced process timing by 86%</strong> (from 14 to two days) between network and internal data systems, creating robust foundation for precise financial reporting, by designing daily reconciliation infrastructure using SQL and Python scripts. 
                    Built ad hoc solutions for the settlements team focusing on network-side transaction processing.</li>
                <li>Developed comprehensive technical documentation mapping chargeback lifecycle in 20-page internal and client-facing guide for processing and reporting, 
                    establishing standardized procedures and technical specifications for ongoing operations.</li>
            </ul>

            <h3>comScore</h3>
            <p><strong>Custom Analytics</strong> - San Francisco, CA</p>
            <p><i>Associate Data Analyst (Q3 2017 - Q3 2018)</i></p>
            <p>Supported analytics initiatives for major technology clients through dashboard development, web data extraction, 
                and custom reporting solutions that delivered actionable business intelligence.</p>
            <ul>
                <li>Generated and maintained analytics dashboards for 10 monthly reporting packages detailing online visitation metrics and market trends for global tech clients. 
                    Delivered actionable BI through Excel-based visualizations and QlikSense dashboards.</li>
                <li>Improved data processing capabilities by producing automated data collection systems using Mozenda web scraping agents and SQL Server scripts 
                    to efficiently classify and analyze data from 1M+ web pages. </li>
                <li>Created custom data processing scripts for eCommerce analysis in comScore's Multi-Panel Environment, 
                    evaluating project feasibility and implementing Bash scripting solutions to address complex analytical challenges.</li>
            </ul>

            <h3>Graduate Teaching Assistant</h3>
            <p><strong>The College of William & Mary</strong> - Williamsburg, VA</p>
            <p><i>Graduate Teaching Assistant (2016-2017)</i></p>
            <ul>
                <li>Taught students to use TD Ameritrade's <i>ThinkorSwim</i> to handle $200,000 paper money trading portfolios.</li>
                <li>Led weekly review sessions on the Black-Scholes Model, Stochastic Calculus, Spectral Analysis, and other topics.</li>
                <li>Evaluated and improved student performance in office hours on problem sets, tests, and final trading portfolios.</li>
            </ul>

            <h3>DC Internship</h3>
            <p><strong>Center for Trade & Economics</strong> - Washington, D.C.</p>
            <p><i>Research Intern (Summer 2015)</i></p>
            <ul>
                <li>Analyzed investment data for 186 countries for the <i>Index of Economic Freedom</i>.</li>
                <li>Wrote, edited, and researched economic policy publications including blogs and Issue Briefs.</li>
                <li>Organized key data in Excel on labor productivity, tax rates, government debt, and international investment</li>
            </ul>
        </section>
        <hr/>
        <section>
            <h2>Projects</h2>
            <h3>Political Consulting</h3>
            <p><strong>Florida Senate Campaign</strong> - Miami, FL</p>
            <p><i>Technical Consultant (Fall 2017)</i></p>
            <ul>
                <li>Developed a comprehensive data model by building a custom database of 400+ voter zip codes and early voting locations in Florida's 12 largest counties.</li>
                <li>Increased early voter turnout by 50% from 6% to 9% by using GIS techniques in Python against custom voter database.</li>
                <li>Wrote 10-page Recruitment Handbook now used nationwide to guide new college chapter's recruitment efforts for similar campaigns.</li>
            </ul>
        </section>
        <hr/>
        
    </main>
</body>
<footer>
    <p><a href="./about.html">About Me</a></p>
    <p><a href="./contact.html">Contact Me</a></p>
</footer>
</body>
